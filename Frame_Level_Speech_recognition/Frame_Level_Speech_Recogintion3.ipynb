{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Environment (conda_pytorch_latest_p36)",
      "language": "python",
      "name": "conda_pytorch_latest_p36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Frame_Level_Speech_Reco3.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GZDPcHAbP6J"
      },
      "source": [
        "# Frame Level Speech Recognition with Neural Networks\n",
        "## IDC410 Machine Learning Assignment\n",
        "### Submitted to Prof. Sarab Anand\n",
        "#### Question for the Assesment\n",
        "In this coursework you will take your knowledge of feedforward neural networks and apply it to the task of speech recognition.\n",
        "\n",
        "You are provided a dataset of audio recordings (utterances) and their phoneme state (subphoneme) labels. The data comes from articles published in the Wall Street Journal (WSJ) that are read aloud and labelled using the original text. If you have not encountered speech data before or have not heard of phonemes or spectrograms, we will clarify these here:\n",
        "\n",
        "#### Phonems and Phoneme States\n",
        "As letters are the atomic elements of written language, phonemes are the atomic elements of speech. It is crucial for us to have a means to distiguish different sounds in speech that may or may not represent the same letter or combinations of letters in the written alphabet. For example, the words \"jet\" and \"ridge\" both contain the same sound and we refer to this elemental sound as the phoneme \"JH\". For this challenge we will consider 46 phonemes in the english language.\n",
        "\n",
        "[\"+BREATH+\", \"+COUGH+\", \"+NOISE+\", \"+SMACK+\", \"+UH+\", \"+UM+\", \"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\", \"B\", \"CH\", \"D\", \"DH\", \"EH\", \"ER\", \"EY\", \"F\", \"G\", \"HH\", \"IH\", \"IY\", \"JH\", \"K\", \"L\", \"M\", \"N\", \"NG\", \"OW\", \"OY\", \"P\", \"R\", \"S\", \"SH\", \"SIL\", \"T\", \"TH\", \"UH\", \"UW\", \"V\", \"W\", \"Y\", \"Z\", \"ZH\"]\n",
        "\n",
        "A powerful technique in speech recognition is to model speech as a markov process with unobserved states. This model considers observed speech to be dependent on unobserved state transitions. We refer to these unobserved states as phoneme states or subphonemes. For each phoneme, there are 3 respective phoneme states. Therefore for our 46 phonemes, there exist 138 respective phoneme states.\n",
        "\n",
        "Hidden Markov Models (HMMs) estimate the parameters of this unobserved markov process (transition and emission probabilities) that maximize the likelihood of the observed speech data.\n",
        "\n",
        "Your task is to instead take a model-free approach and classify mel spectrogram frames using a neural network that takes a frame (plus optional context) and outputs class probabilities for all 138 phoneme states. Performance on the task will be measured by classification accuracy on a held-out set of labelled mel spectrogram frames. Training/dev labels are provided as integers [0-137].\n",
        "\n",
        "#### Representing Speech\n",
        "As a first step, the speech must be converted into a feature representation that can be fed into the network.\n",
        "\n",
        "In our representation, utterances have been converted to \"mel spectrograms\", which are pictorial representations that characterize how the frequency content of the signal varies with time. The frequency-domain of the audio signal provides more useful features for distinguishing phonemes.\n",
        "\n",
        "For a more intuitive understanding, consider attempting to determine which instruments are playing in an orchestra given an audio recording of a performance. By looking only at the amplitude of the signal of the orchestra over time, it is nearly impossible to distinguish one source from another. But if the signal is transformed into the frequency domain, we can use our knowledge that flutes produce higher frequency sounds and bassoons produce lower frequency sounds. In speech, a similar phenomenon is observed when the vocal tract produces sounds at varying frequencies.\n",
        "\n",
        "To convert the speech to a mel spectrogram, it is segmented into little \"frames\", each 25ms wide, where the \"stride\" between adjacent frames is 10ms. Thus we get 100 such frames per second of speech.\n",
        "\n",
        "From each frame, we compute a single \"mel spectral\" vector, where the components of the vector represent the (log) energy in the signal in different frequency bands. In the data we have given you, we have 40-dimensional mel-spectral vectors, i.e. we have computed energies in 40 frequency bands.\n",
        "\n",
        "Thus, we get 100 40-dimensional mel spectral (row) vectors per second of speech in the recording. Each one of these vectors is referred to as a frame. The details of how mel spectrograms are computed from speech is explained in the attached blog.\n",
        "\n",
        "Thus, for a T-second recording, the entire spectrogram is a 100T x 40 matrix, comprising 100T 40- dimensional vectors (at 100 vectors (frames) per second).\n",
        "\n",
        "###### The Training Data Comprises :\n",
        "\n",
        "<li>Speech Recordings\n",
        "<li>Frame Level Phoneme State labels\n",
        "\n",
        "###### The test data comprises\n",
        "\n",
        "<li>Speech Recordings\n",
        "<li>Phoneme state labels are not given\n",
        "\n",
        "\n",
        "#### Expected from Us\n",
        "Your job is to identify the phoneme state label for each frame in the test data set. It is important to note that utterances are of variable length. We are providing you code to load and parse the raw files into the expected format. For now we are only providing dev data files as the training file is very large.\n",
        "\n",
        "Dataset\n",
        "Feature File\n",
        "[train|dev|test].npy contain a numpy object array of shape [utterances]. Each utterance is a float32 ndarray of shape [time, frequency], where time is the length of the utterance. Frequency dimension is always 40 but time dimension is of variable length.\n",
        "\n",
        "Label Files\n",
        "[train|dev]_labels.npy contain a numpy object array of shape [utterances]. Each element in the array is an int32 array of shape [time] and provides the phoneme state label for each frame. There are 138 distinct labels [0-137], one for each subphoneme."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAQ3pZiHZB4C"
      },
      "source": [
        "# Import Files\n",
        "\n",
        "Importing all the necessary files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGELzsPgZB4j"
      },
      "source": [
        "import numpy as np                # numpy for basic matrix operations\n",
        "import os                         # operating system, to use files from my PC\n",
        "import torch                      # PyTorch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import time                       # time library \n",
        "from datetime import datetime\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import argparse                    # argument parser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9f3kR3OZB4l"
      },
      "source": [
        "# Train, Validation, Test NumPy files loading\n",
        "\n",
        "\n",
        "Loading the test , train files directly from my PC(not using the google drive feature here)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2cv5Wp_ZB4p"
      },
      "source": [
        "class LibriSpeech():\n",
        "\n",
        "    def __init__(self, data_path):\n",
        "        self.data_path = data_path\n",
        "        self.dev_set = None\n",
        "        self.train_set = None\n",
        "        self.test_set = None\n",
        "  \n",
        "    @property\n",
        "    def dev(self):\n",
        "        if self.dev_set is None:\n",
        "            self.dev_set = load_data(self.data_path, 'dev')\n",
        "        return self.dev_set\n",
        "\n",
        "    @property\n",
        "    def train(self):\n",
        "        if self.train_set is None:\n",
        "            self.train_set = load_data(self.data_path, 'train')\n",
        "        return self.train_set\n",
        "  \n",
        "    @property\n",
        "    def test(self):\n",
        "        if self.test_set is None:\n",
        "            self.test_set = (np.load(os.path.join(self.data_path, 'test.npy'), encoding='bytes', allow_pickle=True), None)\n",
        "        return self.test_set\n",
        "\n",
        "    \n",
        "def load_data(path, name):\n",
        "    return (\n",
        "        np.load(os.path.join(path, '{}.npy'.format(name)), encoding='bytes', allow_pickle=True),\n",
        "        np.load(os.path.join(path, '{}_labels.npy'.format(name)), encoding='bytes', allow_pickle=True)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0CQT-MzZB4p"
      },
      "source": [
        "# DataSet Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPQl3ZnDZB4q"
      },
      "source": [
        "class MyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, librispeech, k = 15, lowest=0.1):\n",
        "        self.k = k\n",
        "        self.x_list = librispeech[0]\n",
        "        self.y_list = librispeech[1] if len(librispeech) == 2 else None\n",
        "        self.idx_map = []\n",
        "        for i, xs in enumerate(self.x_list):\n",
        "            for j in range(xs.shape[0]):\n",
        "                self.idx_map.append((i, j))\n",
        "        \n",
        "        self.win_mask = np.concatenate((np.arange(lowest, 1.0, (1 - lowest)/k),\n",
        "                            np.arange(1.0, lowest, -(1 - lowest)/k),\n",
        "                            np.array([0.1])))\n",
        "\n",
        "        # self.win_mask = np.concatenate( (np.zeros(k), np.array([1.]), np.zeros(k)), axis=None )\n",
        "        self.win_mask = np.repeat(self.win_mask, librispeech[0][0].shape[1])\n",
        "        \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i, j = self.idx_map[idx]\n",
        "        context = self.x_list[i].take(range(j - self.k, j + self.k + 1), mode='clip', axis=0).flatten()\n",
        "        context *= self.win_mask\n",
        "        xi = torch.from_numpy(context).float()\n",
        "        yi = self.y_list[i][j] if self.y_list is not None else -1\n",
        "        return xi, yi\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwuR2FbLZB4r"
      },
      "source": [
        "# Xavier Initialization\n",
        "\n",
        "Xavier initialization, originally proposed by Xavier Glorot and Yoshua Bengio in \"Understanding the difficulty of training deep feedforward neural networks\", is the weights initialization technique that tries to make the variance of the outputs of a layer to be equal to the variance of its inputs. This idea turned out to be very useful in practice. Naturally, this initialization depends on the layer activation function. And in their paper, Glorot and Bengio considered logistic sigmoid activation function, which was the default choice at that moment.\n",
        "\n",
        "Later on, the sigmoid activation was surpassed by ReLu, because it allowed to solve vanishing / exploding gradients problem. Consequently, there appeared a new initialization technique, which applied the same idea (balancing of the variance of the activation) to this new activation function. It was proposed by Kaiming He at al in \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\", and now it often referred to as He initialization.\n",
        "\n",
        "In tensorflow, He initialization is implemented in variance_scaling_initializer() function (which is, in fact, a more general initializer, but by default performs He initialization), while Xavier initializer is logically xavier_initializer()\n",
        "\n",
        "The initialization works better for layers with ReLu activation.\n",
        "Xavier initialization works better for layers with sigmoid activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax-t3k71ZB4s"
      },
      "source": [
        "def init_xavier(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        m.bias.data.fill_(0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kIRzTC7ZB4s"
      },
      "source": [
        "# Model\n",
        "\n",
        "Making and specifying the neural layers and all the parameters for our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQB0LzwRZB4t"
      },
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        k = 15\n",
        "        in_size = ((k * 2) + 1) * 13\n",
        "        out_size = 346\n",
        "\n",
        "        layers = []\n",
        "        size_list = [in_size, in_size, 1024,  2048, 2048, 1024, 512, out_size, out_size]\n",
        "\n",
        "        for i in range(len(size_list) - 2):\n",
        "            layers.append(nn.Linear(size_list[i],size_list[i+1]))\n",
        "            layers.append(nn.BatchNorm1d(size_list[i+1]))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(0.2))\n",
        "\n",
        "        layers.append(nn.Linear(size_list[-2], size_list[-1]))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        print(self.net)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CtvBqFlZB4u"
      },
      "source": [
        "def get_model(k):\n",
        "    in_size = ((k * 2) + 1) * 13\n",
        "    out_size = 346\n",
        "\n",
        "    layers = []\n",
        "    size_list = [in_size, in_size, 1024,  2048, 2048, 1024, 512, out_size, out_size]\n",
        "\n",
        "    for i in range(len(size_list) - 2):\n",
        "        layers.append(nn.Linear(size_list[i],size_list[i+1]))\n",
        "        layers.append(nn.BatchNorm1d(size_list[i+1]))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Dropout(0.2))\n",
        "        \n",
        "    layers.append(nn.Linear(size_list[-2], size_list[-1]))\n",
        "    mynet = nn.Sequential(*layers)\n",
        "    print(mynet)\n",
        "    return mynet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ke05XE9ZB4w"
      },
      "source": [
        "# Train and Test Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK6l_MvKZB4w"
      },
      "source": [
        "def train(epoch, model, optimizer, train_loader, scheduler, args):\n",
        "    model.train()\n",
        "    \n",
        "    t0 = time.time()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        if args.cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args.log_interval == 0:\n",
        "            print('Train Epoch: {} Batch: {} [{}/{} ({:.0f}%, time:{:.2f}s)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), time.time() - t0,\n",
        "                loss.data))\n",
        "            t0 = time.time()\n",
        "    #scheduler.step()\n",
        "\n",
        "def test(model, test_loader, args):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        if args.cuda:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        test_loss += F.cross_entropy(output, target, size_average=False).data # sum up batch loss\n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return \"{:.4f}%\".format(100. * correct / len(test_loader.dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmvfsxhCZB4y"
      },
      "source": [
        "# Arguments\n",
        "\n",
        "Setting all the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-ljvOfvZB4y"
      },
      "source": [
        "class Argument():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 256     # defining batch size\n",
        "        self.epochs = 29          # number of iterations\n",
        "        self.lr = 0.001           # the learning rate\n",
        "        self.cuda = True          # The availability of GPU\n",
        "        self.data_dir = \"./data/\" # which directory to get data from\n",
        "        self.K = 15\n",
        "        self.seed = 1001\n",
        "        self.momentum = 0.9\n",
        "        self.log_interval = 1000\n",
        "        self.weights_dir = \"./weights/\"\n",
        "        \n",
        "args = Argument()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7brNReK7ZB43"
      },
      "source": [
        "# Dataloading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3-eiMN9ZB44"
      },
      "source": [
        "torch.cuda.manual_seed(args.seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaxxIYPRZB45"
      },
      "source": [
        "librispeech_loader = LibriSpeech(args.data_dir)\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True, 'drop_last': True} if args.cuda else {}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    MyDataset(librispeech_loader.train, k=args.K),\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    MyDataset(librispeech_loader.dev, k=args.K),\n",
        "    batch_size=args.batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6ecxVjyZB46"
      },
      "source": [
        "# Model initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znxYyGJYZB46",
        "outputId": "303f2fbc-680a-4ea7-f8a5-1ce8818aefd8"
      },
      "source": [
        "\n",
        "model = get_model(args.K)\n",
        "model.apply(init_xavier)  # applying Xavier weight initialization\n",
        "if args.cuda:\n",
        "    model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Linear(in_features=403, out_features=403, bias=True)\n",
            "  (1): BatchNorm1d(403, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU()\n",
            "  (3): Dropout(p=0.2, inplace=False)\n",
            "  (4): Linear(in_features=403, out_features=1024, bias=True)\n",
            "  (5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (6): ReLU()\n",
            "  (7): Dropout(p=0.2, inplace=False)\n",
            "  (8): Linear(in_features=1024, out_features=2048, bias=True)\n",
            "  (9): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (10): ReLU()\n",
            "  (11): Dropout(p=0.2, inplace=False)\n",
            "  (12): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "  (13): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (14): ReLU()\n",
            "  (15): Dropout(p=0.2, inplace=False)\n",
            "  (16): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "  (17): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (18): ReLU()\n",
            "  (19): Dropout(p=0.2, inplace=False)\n",
            "  (20): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (21): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (22): ReLU()\n",
            "  (23): Dropout(p=0.2, inplace=False)\n",
            "  (24): Linear(in_features=512, out_features=346, bias=True)\n",
            "  (25): BatchNorm1d(346, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (26): ReLU()\n",
            "  (27): Dropout(p=0.2, inplace=False)\n",
            "  (28): Linear(in_features=346, out_features=346, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc-UqjHpZB4-"
      },
      "source": [
        "# Optimizer and Scheduler\n",
        "\n",
        "## Adam Optimizer\n",
        "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems. Adam is relatively easy to configure where the default configuration parameters do well on most problems.\n",
        "\n",
        "## StepLR\n",
        "Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with other changes to the learning rate from outside this scheduler. When last_epoch=-1, sets initial lr as lr."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpqqVHs_ZB4-"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xH5ZnZNZB4_"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAVoJKQ1ZB5B"
      },
      "source": [
        "for epoch in range(1, args.epochs + 1):\n",
        "    print(datetime.now())\n",
        "    print('LR: ', scheduler.get_last_lr())\n",
        "    train(epoch, model, optimizer, train_loader, scheduler, args)\n",
        "    acc_str = test(model, test_loader, args)\n",
        "    if not os.path.exists(args.weights_dir):\n",
        "        os.makedirs(args.weights_dir)\n",
        "    torch.save(model.state_dict(), \"{}/data_{:03d}.pth\".format(args.weights_dir, epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OD4Jg2-ZB5D"
      },
      "source": [
        "## Load best weight\n",
        "#### ** Please note that you need to load model first (Model Initialization) before executing the below steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eza1AAs3ZB5E",
        "outputId": "0bb42e6b-f7f6-4578-dc15-ed9285ae4b63"
      },
      "source": [
        "\n",
        "model.load_state_dict(torch.load(args.weights_dir+'/data_029.pth'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EJT8w-7ZB5E"
      },
      "source": [
        "## Evaluate Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-WrIn0rZB5F"
      },
      "source": [
        "def eval_model(model, test_loader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        pred = []\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):   \n",
        "            data = data.cuda()\n",
        "            outputs = model(data)\n",
        "            predicted = outputs.data.max(1, keepdim=True)[1]\n",
        "            pred.append(predicted.cpu().numpy()[0])\n",
        "\n",
        "        return np.array(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6tVehauZB5F"
      },
      "source": [
        "## Test Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbponDIoZB5G"
      },
      "source": [
        "librispeech_loader = LibriSpeech(args.data_dir)\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
        "\n",
        "eval_loader = torch.utils.data.DataLoader(\n",
        "    MyDataset(librispeech_loader.test, k=args.K),\n",
        "    batch_size=1, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbQWgBYUZB5G"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt0uAvSwZB5G"
      },
      "source": [
        "pred = eval_model(model, eval_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpLwBO3BZB5G"
      },
      "source": [
        "## Save Prediction for Assignment Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fSMXo21ZB5H"
      },
      "source": [
        "with open('monitsharma.csv', 'w') as w:\n",
        "    w.write('id,label\\n')\n",
        "    for i in range(len(pred)):\n",
        "            w.write(str(i)+','+str(pred[i][0])+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}