{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Frame_level_Speech_Recogintion.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kISmgMWKJSXY"
      },
      "source": [
        "# Frame Level Speech Recognition with Neural Networks\n",
        "\n",
        "## IDC410 Machine Learning Assignment\n",
        "\n",
        "### Submitted to Prof. Sarab Anand"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW9x9uZYJSOf"
      },
      "source": [
        "## Question for the Assesment\n",
        "In this coursework you will take your knowledge of feedforward neural networks and apply it to the task of speech recognition.\n",
        "\n",
        "You are provided a dataset of audio recordings (utterances) and their phoneme state (subphoneme) labels. The data comes from articles published in the Wall Street Journal (WSJ) that are read aloud and labelled using the original text. If you have not encountered speech data before or have not heard of phonemes or spectrograms, we will clarify these here:\n",
        "\n",
        "## Phonems and Phoneme States\n",
        "\n",
        "As letters are the atomic elements of written language, phonemes are the atomic elements of speech. It is crucial for us to have a means to distiguish different sounds in speech that may or may not represent the same letter or combinations of letters in the written alphabet. For example, the words \"jet\" and \"ridge\" both contain the same sound and we refer to this elemental sound as the phoneme \"JH\". For this challenge we will consider 46 phonemes in the english language.\n",
        "\n",
        "[\"+BREATH+\", \"+COUGH+\", \"+NOISE+\", \"+SMACK+\", \"+UH+\", \"+UM+\", \"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\", \"B\", \"CH\", \"D\", \"DH\", \"EH\", \"ER\", \"EY\", \"F\", \"G\", \"HH\", \"IH\", \"IY\", \"JH\", \"K\", \"L\", \"M\", \"N\", \"NG\", \"OW\", \"OY\", \"P\", \"R\", \"S\", \"SH\", \"SIL\", \"T\", \"TH\", \"UH\", \"UW\", \"V\", \"W\", \"Y\", \"Z\", \"ZH\"]\n",
        "\n",
        "A powerful technique in speech recognition is to model speech as a markov process with unobserved states. This model considers observed speech to be dependent on unobserved state transitions. We refer to these unobserved states as phoneme states or subphonemes. For each phoneme, there are 3 respective phoneme states. Therefore for our 46 phonemes, there exist 138 respective phoneme states.\n",
        "\n",
        "Hidden Markov Models (HMMs) estimate the parameters of this unobserved markov process (transition and emission probabilities) that maximize the likelihood of the observed speech data.\n",
        "\n",
        "Your task is to instead take a model-free approach and classify mel spectrogram frames using a neural network that takes a frame (plus optional context) and outputs class probabilities for all 138 phoneme states. Performance on the task will be measured by classification accuracy on a held-out set of labelled mel spectrogram frames. Training/dev labels are provided as integers [0-137].\n",
        "\n",
        "\n",
        "## Representing Speech\n",
        "\n",
        "As a first step, the speech must be converted into a feature representation that can be fed into the network.\n",
        "\n",
        "In our representation, utterances have been converted to \"mel spectrograms\", which are pictorial representations that characterize how the frequency content of the signal varies with time. The frequency-domain of the audio signal provides more useful features for distinguishing phonemes.\n",
        "\n",
        "For a more intuitive understanding, consider attempting to determine which instruments are playing in an orchestra given an audio recording of a performance. By looking only at the amplitude of the signal of the orchestra over time, it is nearly impossible to distinguish one source from another. But if the signal is transformed into the frequency domain, we can use our knowledge that flutes produce higher frequency sounds and bassoons produce lower frequency sounds. In speech, a similar phenomenon is observed when the vocal tract produces sounds at varying frequencies.\n",
        "\n",
        "To convert the speech to a mel spectrogram, it is segmented into little \"frames\", each 25ms wide, where the \"stride\" between adjacent frames is 10ms. Thus we get 100 such frames per second of speech.\n",
        "\n",
        "From each frame, we compute a single \"mel spectral\" vector, where the components of the vector represent the (log) energy in the signal in different frequency bands. In the data we have given you, we have 40-dimensional mel-spectral vectors, i.e. we have computed energies in 40 frequency bands.\n",
        "\n",
        "Thus, we get 100 40-dimensional mel spectral (row) vectors per second of speech in the recording. Each one of these vectors is referred to as a frame. The details of how mel spectrograms are computed from speech is explained in the attached blog.\n",
        "\n",
        "Thus, for a T-second recording, the entire spectrogram is a 100T x 40 matrix, comprising 100T 40- dimensional vectors (at 100 vectors (frames) per second).\n",
        "\n",
        "The Training Data Comprises :\n",
        "\n",
        "<li>Speech Recordings\n",
        "<li>Frame Level Phoneme State labels\n",
        "The test data comprises\n",
        "\n",
        "<li>Speech Recordings\n",
        "<li>Phoneme state labels are not given\n",
        "\n",
        "\n",
        "\n",
        "### Expected from Us\n",
        "\n",
        "Your job is to identify the phoneme state label for each frame in the test data set. It is important to note that utterances are of variable length. We are providing you code to load and parse the raw files into the expected format. For now we are only providing dev data files as the training file is very large\n",
        "\n",
        "### Dataset\n",
        "\n",
        "#### Feature File\n",
        "[train|dev|test].npy contain a numpy object array of shape [utterances]. Each utterance is a float32 ndarray of shape [time, frequency], where time is the length of the utterance. Frequency dimension is always 40 but time dimension is of variable length\n",
        "\n",
        "\n",
        "#### Label File\n",
        "\n",
        "[train|dev]_labels.npy contain a numpy object array of shape [utterances]. Each element in the array is an int32 array of shape [time] and provides the phoneme state label for each frame. There are 138 distinct labels [0-137], one for each subphoneme.\n",
        "\n",
        "You can downlaoad the dataset from [here](https://www.kaggle.com/c/cmu-11785-deep-learning-hw1-p2/data)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oByFqQe2JSCB"
      },
      "source": [
        "Importing all the basic and ML Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1oHN3FMk6JV"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjvTlVdJk72E"
      },
      "source": [
        "import torch\n",
        "cuda = torch.cuda.is_available()\n",
        "print(cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R6C37cXKLyo"
      },
      "source": [
        "## Implementation\n",
        "The dataset files are of nearly 8GB size, We can't load them directly to google colab notebook, instead we make use of Google Drive.\n",
        "\n",
        "Upload the files on Google Drive and make use of Drive feature of the google colaboratry, type the below code, It'll show you a link, Visit that link, give confirmation, copy the auth code and paste it in the dialog box that appears. It will let you access the files in your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V5db1f_k9Ws"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nZJ0IEllDH_"
      },
      "source": [
        "\n",
        "train_labels = np.load('/content/drive/My Drive/train_labels.npy',allow_pickle=True)\n",
        "dev_train = np.load('/content/drive/My Drive/dev.npy',allow_pickle=True)\n",
        "dev_labels = np.load('/content/drive/My Drive/dev_labels.npy',allow_pickle=True)\n",
        "test =  np.load('/content/drive/My Drive/test.npy',allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPgQOvnLlNiY"
      },
      "source": [
        "train = np.load('/content/drive/My Drive/train.npy',allow_pickle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZclLnH-lQCP"
      },
      "source": [
        "# Checking the Shape of the training Data\n",
        "train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xefj9E01KoSU"
      },
      "source": [
        "### TORCH.UTILS.DATA\n",
        "\n",
        "At the heart of PyTorch data loading utility is the torch.utils.data.DataLoader class. It represents a Python iterable over a dataset, with support for\n",
        "<li> map-style and iterable-style datasets,\n",
        "<li> customizing data loading order,\n",
        "<li> automatic batching,\n",
        "<li> single- and multi-process data loading,\n",
        "<li> automatic memory pinning.\n",
        "\n",
        "\n",
        "#### DataLoader\n",
        "           Dataloader(dataset, batch_size=1, shuffle=False, sampler=None,\n",
        "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
        "           pin_memory=False, drop_last=False, timeout=0,\n",
        "           worker_init_fn=None, *, prefetch_factor=2,\n",
        "           persistent_workers=False)\n",
        "\n",
        "The most important argument of DataLoader constructor is dataset, which indicates a dataset object to load data from.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzEWQdcBl4bo"
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "class TensorDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        super().__init__()\n",
        "        assert len(x) == len(y)\n",
        "        self._x = x\n",
        "        self._y = y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self._x)\n",
        "      \n",
        "    def __getitem__(self, index):\n",
        "        x_item = self._x[index]\n",
        "        return torch.FloatTensor(x_item), torch.FloatTensor(self._y[index])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW4ircIXMCFl"
      },
      "source": [
        "#### TensorDataset\n",
        "A dataset of tensors.\n",
        "\n",
        "Stores a single tensor internally, which is then indexed inside get()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFRed7G2l8J_"
      },
      "source": [
        "train_dataset = TensorDataset(train, train_labels)\n",
        "\n",
        "load_train = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = 1,\n",
        "    shuffle=False,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "dev_dataset = TensorDataset(dev_train, dev_labels)\n",
        "\n",
        "load_valid = DataLoader(\n",
        "    dev_dataset,\n",
        "    batch_size = 1\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKsYzWzdmDKK"
      },
      "source": [
        "\n",
        "import torch\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8xnkyqGM0ek"
      },
      "source": [
        "To work with GPU in Pytorch you have to make sure the model and the tensors to be referred to the GPU or cuda else it will show an error. Also, colab provides only around 11GB. You can also use pin_memory while loading the dataset if your dataset preprocessing can directly be done on the GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExGgbsUPmMnN"
      },
      "source": [
        "\n",
        "DEVICE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp_xL640mOGR"
      },
      "source": [
        "embedding_dim = 40          # the dimensions used in the Neural Network\n",
        "hidden_dim = 10             # the number of hidden layer dimensions\n",
        "vocab_size = 138            # It goes from [0-137], hence 138\n",
        "layers=4                    # the total number of layers used to make the neural network\n",
        "\n",
        "def hidden_init():\n",
        "    return (torch.rand(layers*2, 1, hidden_dim).to(DEVICE) ,\n",
        "            torch.rand(layers*2, 1, hidden_dim).to(DEVICE))\n",
        "\n",
        "hidden_init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvxj9RDXOvcx"
      },
      "source": [
        "## LSTM Model\n",
        "\n",
        "It is special kind of recurrent neural network that is capable of learning long term dependencies in data. This is achieved because the recurring module of the model has a combination of four layers interacting with each other.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPoM9SfDPKQr"
      },
      "source": [
        "%pylab inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "img = mpimg.imread('LSTM.jpg')\n",
        "imgplot = plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77F8ZVtoQDgB"
      },
      "source": [
        "The picture above depicts four neural network layers in yellow boxes, point wise operators in green circles, input in yellow circles and cell state in blue circles. An LSTM module has a cell state and three gates which provides them with the power to selectively learn, unlearn or retain information from each of the units. The cell state in LSTM helps the information to flow through the units without being altered by allowing only a few linear interactions. Each unit has an input, output and a forget gate which can add or remove the information to the cell state. The forget gate decides which information from the previous cell state should be forgotten for which it uses a sigmoid function. The input gate controls the information flow to the current cell state using a point-wise multiplication operation of ‘sigmoid’ and ‘tanh’ respectively. Finally, the output gate decides which information should be passed on to the next hidden state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMvWy9HdmQzN"
      },
      "source": [
        "class LSTM_model(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LSTM_model, self).__init__()\n",
        "        self.vocab_size = 138                 # vocab_size\n",
        "        self.embedding_dim = embedding_dim    # embedding dimension\n",
        "        self.hidden_dim = hidden_dim          # hidden dimension\n",
        "        \n",
        "        self.lstm = torch.nn.LSTM(embedding_dim, hidden_dim, num_layers=layers, dropout = 0.2, bidirectional = True).to(DEVICE)\n",
        "        self.linear = torch.nn.Linear(hidden_dim*2, vocab_size)       # *2 applied if bidir = true\n",
        "        self.softmax = torch.nn.functional.softmax\n",
        "        \n",
        "    def forward(self, encrypted):\n",
        "        lstm_in = encrypted.transpose(0,1)\n",
        "\n",
        "        lstm_out, lstm_hidden = self.lstm(lstm_in.float(), hidden_init())\n",
        "        \n",
        "        scores = self.linear(lstm_out)\n",
        "        scores = scores.transpose(1, 2)\n",
        "\n",
        "        return scores\n",
        "\n",
        "model = LSTM_model(vocab_size, embedding_dim, hidden_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlB4siXfQj2H"
      },
      "source": [
        "When loading a model on a GPU that was trained and saved on GPU, simply convert the initialized model to a CUDA optimized model using model.to(torch.device('cuda')). Also, be sure to use the .to(torch.device('cuda')) function on all model inputs to prepare the data for the model. Note that calling my_tensor.to(device) returns a new copy of my_tensor on GPU. It does NOT overwrite my_tensor. Therefore, remember to manually overwrite tensors: my_tensor = my_tensor.to(torch.device('cuda')).\n",
        "\n",
        "\n",
        "model.cuda() and model.to(device) are the same, but they actually gave different running time.\n",
        "They do the same thing yes: send each param to the GPU one after the other.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZngzVmtcmYF0"
      },
      "source": [
        "\n",
        "model = model.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7Zq9OhHmZzw"
      },
      "source": [
        "#  the trained Model\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/model_5.sav'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n59nnR4-Sarx"
      },
      "source": [
        "### Cross Entropy Loss\n",
        "This criterion combines LogSoftmax and NLLLoss in one single class.It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes.This is particularly useful when you have an unbalanced training set.\n",
        "\n",
        "### Adam Optimizer\n",
        "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems. Adam is relatively easy to configure where the default configuration parameters do well on most problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZCka6n0nB9z"
      },
      "source": [
        "# making the LSTM trainer\n",
        "losses = []        # empty array for the losses to be appended \n",
        "\n",
        "class LSTM_Trainer():\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.loss_fn = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
        "\n",
        "    def get_loss(self, encrypted, original) :\n",
        "        encrypted = encrypted.to(DEVICE).long()\n",
        "        original = original.to(DEVICE).long()\n",
        "\n",
        "        scores = self.model.forward(encrypted)\n",
        "        original = original.transpose(0,1)\n",
        "        original = original.long()\n",
        "\n",
        "        loss = self.loss_fn(scores, original)  # <- Training loss\n",
        "        return loss\n",
        "\n",
        "    def train(self, num_epochs):\n",
        "        accuracies, max_accuracy = [], 0\n",
        "        best_valid_loss = 10   # V.high initialization\n",
        "\n",
        "        with open(os.path.join(PATH, 'history.csv'),'w') as writer:\n",
        "            for N in range(num_epochs):\n",
        "                print('Epoch: {}'.format(N))\n",
        "                for i, (encrypted, original) in enumerate(load_train):  #dataset(num_examples):\n",
        "                    self.optimizer.zero_grad()\n",
        "  \n",
        "                    loss = self.get_loss(encrypted, original)  # <- Training loss\n",
        "                    loss.backward()\n",
        "\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                # Validation\n",
        "                    if i % validation_time == 0:\n",
        "\n",
        "                        print('Validation:' + str(i))\n",
        "                        validation_loss = []\n",
        "                        for (val_encrypted, val_original) in load_valid:    #val dataset(num_examples):\n",
        "                            val_loss = self.get_loss(val_encrypted, val_original) \n",
        "                      \n",
        "                            validation_loss.append(val_loss.item())\n",
        "\n",
        "                        avg_loss = sum(validation_loss) / len(validation_loss)\n",
        "                        print('Training Loss: {:6.4f}'.format(loss.item()))\n",
        "                        print('Validation Loss: {:6.4f}'.format(avg_loss))        \n",
        "                        writer.write(str(N)+','+str(i)+','+str(loss.item())+','+str(avg_loss))\n",
        "                        writer.write('\\n')\n",
        "\n",
        "                # Saving the model after an epoch\n",
        "                model_saved = os.path.join(PATH, 'model_' + str(N+1) + '.sav')\n",
        "                torch.save(self.model.state_dict(), model_saved)\n",
        "\n",
        "                print('Train Loss at end of epoch: {:6.4f}'.format(loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6VkRW-fnFuc"
      },
      "source": [
        "trainer = LSTM_Trainer(model)  # training the model using the LSTM trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g-aAjQjnHIq"
      },
      "source": [
        "n_epochs = 15                  # number of iterations the program will run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-q6UgYEnJCd"
      },
      "source": [
        "\n",
        "# Path to home\n",
        "PATH = os.getcwd()\n",
        "model.load_state_dict(torch.load('/content/drive/My Drive/model_5.sav'))   # getting the saved model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juyZe4sUOM2q"
      },
      "source": [
        "# Printing validation loss at regular intervals\n",
        "validation_time = len(train) / 20\n",
        "print(validation_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inanyjV5PTEt"
      },
      "source": [
        "# Below call starts training models for required epochs\n",
        "\n",
        "trainer.train(n_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_sGWQconLCy"
      },
      "source": [
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        super().__init__()\n",
        "        assert len(x) == len(y)\n",
        "        self._x = x\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self._x)\n",
        "      \n",
        "    def __getitem__(self, index):\n",
        "        x_item = self._x[index]\n",
        "        return torch.FloatTensor(x_item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdH9pTZFoubS"
      },
      "source": [
        "test_dataset = TestDataset(test)\n",
        "\n",
        "load_test = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = 1,\n",
        "    shuffle=False,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS7PlzMpRySA"
      },
      "source": [
        "## Softmax Function\n",
        "Softmax extends the idea of logistic regression into a multi-class world. That is, Softmax assigns decimal probabilities to each class in a multi-class problem. Those decimal probabilities must add up to 1.0. This additional constraint helps training converge more quickly than it otherwise would.\n",
        "\n",
        "Softmax is implemented through a neural network layer just before the output layer. The Softmax layer must have the same number of nodes as the output layer.\n",
        "\n",
        "#### Softmax Options\n",
        "Consider the following variants of Softmax:\n",
        "\n",
        "<li> Full Softmax is the Softmax we've been discussing; that is, Softmax calculates a probability for every possible class.\n",
        "\n",
        "<li> Candidate sampling means that Softmax calculates a probability for all the positive labels but only for a random sample of negative labels. For example, if we are interested in determining whether an input image is a beagle or a bloodhound, we don't have to provide probabilities for every non-doggy example.\n",
        "\n",
        "Full Softmax is fairly cheap when the number of classes is small but becomes prohibitively expensive when the number of classes climbs. Candidate sampling can improve efficiency in problems having a large number of classes.\n",
        "\n",
        "\n",
        "### One Label vs. Many Labels\n",
        "Softmax assumes that each example is a member of exactly one class. Some examples, however, can simultaneously be a member of multiple classes. For such examples:\n",
        "\n",
        "<li>You may not use Softmax.\n",
        "<li>You must rely on multiple logistic regressions.\n",
        "\n",
        "For example, suppose your examples are images containing exactly one item—a piece of fruit. Softmax can determine the likelihood of that one item being a pear, an orange, an apple, and so on. If your examples are images containing all sorts of things—bowls of different kinds of fruit—then you'll have to use multiple logistic regressions instea"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0VQl983o5Sf"
      },
      "source": [
        "soft = torch.nn.Softmax(dim=0)\n",
        "\n",
        "with open('monitsharma.csv') as output:\n",
        "    output.write('id,label')\n",
        "    output_id = 0\n",
        "    for encrypted in load_test:\n",
        "        encrypted = encrypted.to(DEVICE)\n",
        "        scores = model.forward(encrypted)\n",
        "\n",
        "        soft_scores = soft(scores[0])      \n",
        "        predictions = torch.max(soft_scores, 0)   \n",
        "        for prediction in predictions:\n",
        "            output.write(output_id + ',' + prediction)\n",
        "            output_id += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}