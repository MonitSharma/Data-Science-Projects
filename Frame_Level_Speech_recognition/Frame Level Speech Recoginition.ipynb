{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speech.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XqJaMnO0YbP"
      },
      "source": [
        "## Frame Level Speech Recognition with Neural Networks\n",
        "\n",
        "### IDC410 Machine Learning Assesment\n",
        "\n",
        "#### Submitted to Prof Sarab Anand"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLcXjk380sBS"
      },
      "source": [
        "## Question for the Assesment\n",
        "In this coursework you will take your knowledge of feedforward neural networks and apply it to the task of speech recognition.\n",
        "\n",
        "You are provided a dataset of audio recordings (utterances) and their phoneme state (subphoneme) labels. The data comes from articles published in the Wall Street Journal (WSJ) that are read aloud and labelled using the original text. If you have not encountered speech data before or have not heard of phonemes or spectrograms, we will clarify these here:\n",
        "\n",
        "\n",
        "### Phonems and Phoneme States\n",
        "\n",
        "As letters are the atomic elements of written language, phonemes are the atomic elements of speech. It is crucial for us to have a means to distiguish different sounds in speech that may or may not represent the same letter or combinations of letters in the written alphabet. For example, the words \"jet\" and \"ridge\" both contain the same sound and we refer to this elemental sound as the phoneme \"JH\". For this challenge we will consider 46 phonemes in the english language.\n",
        "\n",
        "[\"+BREATH+\", \"+COUGH+\", \"+NOISE+\", \"+SMACK+\", \"+UH+\", \"+UM+\", \"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\", \"B\", \"CH\", \"D\", \"DH\", \"EH\", \"ER\", \"EY\", \"F\", \"G\", \"HH\", \"IH\", \"IY\", \"JH\", \"K\", \"L\", \"M\", \"N\", \"NG\", \"OW\", \"OY\", \"P\", \"R\", \"S\", \"SH\", \"SIL\", \"T\", \"TH\", \"UH\", \"UW\", \"V\", \"W\", \"Y\", \"Z\", \"ZH\"]\n",
        "\n",
        "A powerful technique in speech recognition is to model speech as a markov process with unobserved states. This model considers observed speech to be dependent on unobserved state transitions. We refer to these unobserved states as phoneme states or subphonemes. For each phoneme, there are 3 respective phoneme states. Therefore for our 46 phonemes, there exist 138 respective phoneme states.\n",
        "\n",
        "\n",
        "Hidden Markov Models (HMMs) estimate the parameters of this unobserved markov process (transition and emission probabilities) that maximize the likelihood of the observed speech data. \n",
        "\n",
        "Your task is to instead take a model-free approach and classify mel spectrogram frames using a neural network that takes a frame (plus optional context) and outputs class probabilities for all 138 phoneme states. Performance on the task will be measured by classification accuracy on a held-out set of labelled mel spectrogram frames. Training/dev labels are provided as integers [0-137].\n",
        "\n",
        "\n",
        "### Representing Speech\n",
        "\n",
        "As a first step, the speech must be converted into a feature representation that can be fed into the network.\n",
        "\n",
        "In our representation, utterances have been converted to \"mel spectrograms\", which are pictorial representations that characterize how the frequency content of the signal varies with time. The frequency-domain of the audio signal provides more useful features for distinguishing phonemes.\n",
        "\n",
        "For a more intuitive understanding, consider attempting to determine which instruments are playing in an orchestra given an audio recording of a performance. By looking only at the amplitude of the signal of the orchestra over time, it is nearly impossible to distinguish one source from another. But if the signal is transformed into the frequency domain, we can use our knowledge that flutes produce higher frequency sounds and bassoons produce lower frequency sounds. In speech, a similar phenomenon is observed when the vocal tract produces sounds at varying frequencies.\n",
        "\n",
        "To convert the speech to a mel spectrogram, it is segmented into little \"frames\", each 25ms wide, where the \"stride\" between adjacent frames is 10ms. Thus we get 100 such frames per second of speech.\n",
        "\n",
        "From each frame, we compute a single \"mel spectral\" vector, where the components of the vector represent the (log) energy in the signal in different frequency bands. In the data we have given you, we have 40-dimensional mel-spectral vectors, i.e. we have computed energies in 40 frequency bands.\n",
        "\n",
        "Thus, we get 100 40-dimensional mel spectral (row) vectors per second of speech in the recording. Each one of these vectors is referred to as a frame. The details of how mel spectrograms are computed from speech is explained in the attached blog.\n",
        "\n",
        "Thus, for a T-second recording, the entire spectrogram is a 100T x 40 matrix, comprising 100T 40- dimensional vectors (at 100 vectors (frames) per second).\n",
        "\n",
        "The Training Data Comprises :\n",
        "<li> Speech Recordings\n",
        "<li> Frame Level Phoneme State labels\n",
        "\n",
        "The test data comprises\n",
        "<li> Speech Recordings\n",
        "<li> Phoneme state labels are not given\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn8q4ks21ZNL"
      },
      "source": [
        "### Expected from Us\n",
        "\n",
        "Your job is to identify the phoneme state label for each frame in the test data set. It is important to note that utterances are of variable length. We are providing you code to load and parse the raw files into the expected format. For now we are only providing dev data files as the training file is very large.\n",
        "\n",
        "\n",
        "### Dataset\n",
        "#### Feature File \n",
        "\n",
        "[train|dev|test].npy contain a numpy object array of shape [utterances]. Each utterance is a float32 ndarray of shape [time, frequency], where time is the length of the utterance. Frequency dimension is always 40 but time dimension is of variable length.\n",
        "\n",
        "#### Label Files\n",
        "\n",
        "[train|dev]_labels.npy contain a numpy object array of shape [utterances]. Each element in the array is an int32 array of shape [time] and provides the phoneme state label for each frame. There are 138 distinct labels [0-137], one for each subphoneme.\n",
        "\n",
        "\n",
        "\n",
        "You can downlaoad the dataset from [here](https://www.kaggle.com/c/cmu-11785-deep-learning-hw1-p2/data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd2TEdai2VHU"
      },
      "source": [
        "### Implementation\n",
        "The dataset files are of nearly 8GB size, We can't load them directly to google colab notebook, instead we make use of Google Drive.\n",
        "\n",
        "Upload the files on Google Drive and make use of Drive feature of the google colaboratry, type the below code, It'll show you a link, Visit that link, give confirmation, copy the auth code and paste it in the dialog box that appears.\n",
        "It will let you access the files in your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeAQVFCgDs73"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBZ4Ydr9ahUk"
      },
      "source": [
        "If it shows Mounted at /content/drive \n",
        "Then your google drive has been synched with google colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsrzGDBo4EgR"
      },
      "source": [
        "### Use GPU\n",
        "As the data file is too big, and the ML Model wil require more intense computations, it'll be better to run the whole process on GPU.\n",
        "\n",
        "On the top Click on Runtime, and select Change Runtime , and select GPU.\n",
        "\n",
        "Runtime>>Change Runtine>> GPU\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-jgFae6XXBe"
      },
      "source": [
        "Check the version of the GPU. It's better to run the whole program on Google Colab, since Google Colab let us use GPU and the Google backend to make our program train faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phjfhT_QwoUj"
      },
      "source": [
        "## check the version of CUDA and other deatils about the GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ3n_nPZZUrh"
      },
      "source": [
        "We have the version 11.2 CUDA. Since for Neural Networks we need to work with either tensorflow or pytorch. I am using PyTorch here. \n",
        "To download the best suited version of PyTorch for your system. \n",
        "Visit [here](https://pytorch.org/) \n",
        "\n",
        "Select all the necessary requirements, and your systems specification, copy the text and insert the text below to install PyTorch.\n",
        "\n",
        "!pip install "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGq1FmURw4o4"
      },
      "source": [
        "pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNO9gsQAbFPZ"
      },
      "source": [
        "Since PyTorch is a big library, it'll take some time to install.\n",
        "It'll show\n",
        "Successfully installed torch followed by its version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_evt85axd6H"
      },
      "source": [
        "#importing the basic libraries\n",
        "import numpy as np               # numpy for matrix operation\n",
        "import sys                       # the sys module provides information about constants, functions and methods of the Python interpreter\n",
        "import matplotlib.pyplot as plt  # matplotlib is used to plot the graphs and figures\n",
        "import time                      # the time() function returns the number of seconds passed since epoch, so we import time library\n",
        "\n",
        "# getting the ML libraries to make our neural network\n",
        "import torch                     # getting the PyTorch module imported\n",
        "import torch.nn as nn            # importing the neural network module from PyTorch(torch) by the name nn\n",
        "import torch.nn.functional as F  # torch.nn.functional contains function like convulation layer, pooling function etc\n",
        "import torch.optim as optim      # torch.optim is a package implementing various optimization algorithms.\n",
        "\n",
        "# getting some functions from Pytorch libraries\n",
        "from torch.utils import data        # importing the data func from the pytorch module\n",
        "from torchvision import transforms  # Transforms are common image transformations. They can be chained together using Compose\n",
        "from torch.optim.lr_scheduler import StepLR  # torch.optim is a package implementing various optimization algorithms  \n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau # StepLR Decays the learning rate of each parameter group by gamma every step_size epochs.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5IW33W_4O7W"
      },
      "source": [
        "The ML model we want to train has a training file of 6.1GB , and therefore will take a lot of computing power. The use of GPU is must. Google Colab offers the use of GPU backend and 12GB RAM.\n",
        "PyTorch has a functionality of CUDA to check for GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt00PIFjzyiw"
      },
      "source": [
        "# check for GPU\n",
        "cuda = torch.cuda.is_available() # this line checks whether GPU is available on the device and returns True if it is available\n",
        "cuda\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j1p7FDIz7GE"
      },
      "source": [
        "# Checking the PyTorch version\n",
        "print(torch.__version__)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcZ7uzgJ49fz"
      },
      "source": [
        "## Loading the Dataset\n",
        "\n",
        "Since the data files are too large , and hence can not be directly uploaded to the Google Colab, so we sync the Google drive and upload the data files on Google Drive, adnd use the load function from the numpy library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfS91H530ODb"
      },
      "source": [
        "# loading the data file\n",
        "train_labels = np.load('/content/drive/My Drive/train_labels.npy',allow_pickle=True)  # Allow loading pickled object arrays stored in npy files.\n",
        "dev_train = np.load('/content/drive/My Drive/dev.npy',allow_pickle=True)              # Reasons for disallowing pickles include security\n",
        "dev_labels = np.load('/content/drive/My Drive/dev_labels.npy',allow_pickle=True)      # As loading pickled data can execute arbitrary code. \n",
        "test =  np.load('/content/drive/My Drive/test.npy',allow_pickle=True)                 # If pickles are disallowed, loading object arrays will fail."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhWaQ8TuApmf"
      },
      "source": [
        "# loading the training dataset\n",
        "train = np.load('/content/drive/My Drive/train.npy',allow_pickle=True)     # this file will take longer to load"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heiMoUyI6_Ec"
      },
      "source": [
        "##Dataloader \n",
        "In the dataloader, I have padded the feature vector and stacked both the features and labels as one large 2D array each in the init part. The concatenation of frames is done in the get item part. It might take a long time to load the train data into the train loader depending on the system.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyUSb27H52GJ"
      },
      "source": [
        "# Making a class by the name of MyDataset, that takes on the data parameter\n",
        "\n",
        "class MyDataset(data.Dataset):\n",
        "    def __init__(self, X,Y,k):\n",
        "       \n",
        "        self.X = X               # intitalize all the parameters as self\n",
        "        self.Y = Y\n",
        "        self.k = k\n",
        "        self.samples = []        # initialize empty arrays/lists which will be used further\n",
        "        self.labels = []\n",
        "        self.length = []\n",
        "        self._init_dataset()\n",
        "        self.ind = np.arange(self.length[-1])    \n",
        "        km = [self.k*(2*i+1) for i in range(len(self.length))]\n",
        "        \n",
        "        b = 0\n",
        "        for i in range(self.length[-1]):\n",
        "            if i == self.length[b]:\n",
        "                b = b+1\n",
        "                self.ind[i] = self.ind[i] + km[b]\n",
        "            else:\n",
        "                self.ind[i] = self.ind[i] + km[b]\n",
        "        \n",
        "    # function to find the length of the dataset used after above \n",
        "    def __len__(self):\n",
        "        print(len(self.samples),len(self.labels))\n",
        "        return len(self.labels)\n",
        "    # concatinate the different arrays, we'll get high dimension data, generally called Tensor-\n",
        "    def __getitem__(self,index):\n",
        "        X = np.concatenate((self.samples[self.ind[index]-self.k:self.ind[index]+ self.k+1]),axis=0)\n",
        "        labels = self.labels[index]\n",
        "        return torch.from_numpy(X).float(),torch.tensor(labels).long()\n",
        "    \n",
        "    def _init_dataset(self):\n",
        "        s = 0\n",
        "        for i in range(len(self.X)):\n",
        "            p = np.pad(self.X[i], ((self.k, self.k), (0, 0)), 'constant', constant_values=0)\n",
        "            s = s + len(self.X[i])\n",
        "            self.length.append(s)\n",
        "            self.samples = self.samples + list(p)\n",
        "            self.labels = self.labels + list(self.Y[i]) \n",
        "\n",
        "         \n",
        "        return np.array(self.samples), np.array(self.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlslAUad6P_b"
      },
      "source": [
        "# making the class TestDataset, which have the functions used in the testing phase of our model\n",
        "# its similar to the MyDataset class made above\n",
        "class TestDataset(data.Dataset):\n",
        "    def __init__(self, X,k):\n",
        "       \n",
        "        self.X = X\n",
        "        self.k = k\n",
        "        self.samples = []\n",
        "        self.length = []\n",
        "        self._init_dataset()\n",
        "        self.ind = np.arange(self.length[-1])\n",
        "        km = [self.k*(2*i+1) for i in range(len(self.length))]\n",
        "        \n",
        "        b = 0\n",
        "        for i in range(self.length[-1]):\n",
        "            if i == self.length[b]:\n",
        "                b = b+1\n",
        "                self.ind[i] = self.ind[i] + km[b]\n",
        "            else:\n",
        "                self.ind[i] = self.ind[i] + km[b]\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        print(len(self.samples),self.length[-1])\n",
        "        return self.length[-1]\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        X = np.concatenate((self.samples[self.ind[index]-self.k:self.ind[index]+ self.k+1]),axis=0)\n",
        "        return torch.from_numpy(X).float()\n",
        "    \n",
        "    def _init_dataset(self):\n",
        "        s = 0\n",
        "        for i in range(len(self.X)):\n",
        "            p = np.pad(self.X[i], ((self.k, self.k), (0, 0)), 'constant', constant_values=0)\n",
        "            s = s + len(self.X[i])\n",
        "            self.length.append(s)\n",
        "            self.samples = self.samples + list(p)\n",
        "         \n",
        "        return np.array(self.samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw8WKU9g-SGU"
      },
      "source": [
        "### Difference between num_workers and batch_size\n",
        "\n",
        "Mostly people confuse between num_workers and batch_size. The num_workers is not related to batch_size. Say you set batch_size to 20 and the training size is 2000, then each epoch would contain 100 iterations, i.e. for each iteration, the data loader returns a batch of 20 instances. num_workers > 0 is used to preprocess batches of data so that the next batch is ready for use when the current batch has been finished. More num_workers would consume more memory usage but is helpful to speed up the I/O process.\n",
        "\n",
        "Use of num_workers is advisable only if you're running your device on GPU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lyb_zwYZsanQ"
      },
      "source": [
        "# Having more workers will increase the memory usage and that’s the most serious overhead\n",
        "num_workers = 8 if cuda else 0  # this function set num_workers to 8 if GPU is available, else 0\n",
        "\n",
        "# preparing the model to train\n",
        "train_dataset = MyDataset(train, train_labels,13) \n",
        "\n",
        "train_loader_args = dict(shuffle=True, batch_size=256, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "train_loader = data.DataLoader(train_dataset, **train_loader_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwJiGbTr6TlQ"
      },
      "source": [
        "##Model \n",
        "I have used k=13 (input_size=1080), batch size 0f 256, initialized the model with xavier initialization, used batch norm after activations and used GeLU as activation function (torch 1.4 required). I have used Adam Optimizer with default learning rate and reduced the learning rate by 0.5 after every 5 epochs and ran it for around 30 epochs (I do not exactly remember how many epochs I ran, 30 is a conservative estimate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLv-crib6WAQ"
      },
      "source": [
        "# Validation\n",
        "num_workers = 8 \n",
        "val_dataset = MyDataset(dev, dev_labels,13)\n",
        "val_loader_args = dict(shuffle=False, batch_size=256, num_workers=num_workers, pin_memory=True)\n",
        "val_loader = data.DataLoader(val_dataset, **val_loader_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dARbI3hL6XsI"
      },
      "source": [
        "\n",
        "# Testing\n",
        "test_dataset = TestDataset(test,13)\n",
        "test_loader_args = dict(shuffle=False, batch_size=1, num_workers=num_workers, pin_memory=True)\n",
        "test_loader = data.DataLoader(test_dataset, **test_loader_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tny1jwqZ_YUi"
      },
      "source": [
        "## Xavier Initialization\n",
        "\n",
        "Xavier initialization, originally proposed by Xavier Glorot and Yoshua Bengio in \"Understanding the difficulty of training deep feedforward neural networks\", is the weights initialization technique that tries to make the variance of the outputs of a layer to be equal to the variance of its inputs. This idea turned out to be very useful in practice. Naturally, this initialization depends on the layer activation function. And in their paper, Glorot and Bengio considered logistic sigmoid activation function, which was the default choice at that moment.\n",
        "\n",
        "Later on, the sigmoid activation was surpassed by ReLu, because it allowed to solve vanishing / exploding gradients problem. Consequently, there appeared a new initialization technique, which applied the same idea (balancing of the variance of the activation) to this new activation function. It was proposed by Kaiming He at al in \"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\", and now it often referred to as He initialization.\n",
        "\n",
        "In tensorflow, He initialization is implemented in variance_scaling_initializer() function (which is, in fact, a more general initializer, but by default performs He initialization), while Xavier initializer is logically xavier_initializer()\n",
        "\n",
        "<li> The initialization works better for layers with ReLu activation.\n",
        "<li>  Xavier initialization works better for layers with sigmoid activation\n",
        "\n",
        "Read about it more [here](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) and [here](https://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le0kUmD16ZmI"
      },
      "source": [
        "# the xavier function\n",
        "\n",
        "def init_xavier(m):\n",
        "  if type(m) == nn.Linear:\n",
        "    fan_in = m.weight.size()[1]\n",
        "    fan_out = m.weight.size()[0]\n",
        "    std = np.sqrt(1.0/(fan_in + fan_out))\n",
        "    m.weight.data.normal_(0,std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXKtRxQb6qDJ"
      },
      "source": [
        "def init_hey(m):\n",
        "  if type(m) == nn.Linear:\n",
        "    fan_in = m.weight.size()[1]\n",
        "    fan_out = m.weight.size()[0]\n",
        "    std = np.sqrt(2.0/(fan_in + fan_out))\n",
        "    m.weight.data.normal_(0,std)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdhllwpHCKJX"
      },
      "source": [
        "## Functions Used in Making the Neural Network\n",
        "\n",
        "### nn.Linear()\n",
        "torch.nn.Linear(in_features, out_features, bias=True)\\\n",
        "Applies a linear transformation to the incoming data: $y = xA^T + b$\n",
        "\n",
        "###### Parameters\n",
        "<li> in_features – size of each input sample\n",
        "\n",
        "<li> out_features – size of each output sample\n",
        "\n",
        "<li> bias – If set to False, the layer will not learn an additive bias. Default: True\n",
        "\n",
        "\n",
        "###### Shape\n",
        "<li> Input: $(N, *, H_{in})$  where $*$ means any number of additional dimensions and $H_{in} = \\text{in_features}$\n",
        "\n",
        "<li> Output: $(N, *, H_{out})$\n",
        "where all but the last dimension are the same shape as the input and $H_{out} = \\text{out_features}$\n",
        "\n",
        "\n",
        "### nn.GELU()\n",
        "\n",
        "Applies the Gaussian Error Linear Units function:\n",
        "\n",
        "$\\text{GELU}(x) = x * \\Phi(x)\n",
        "$\n",
        "where $\\Phi(x)$is the Cumulative Distribution Function for Gaussian Distribution.\n",
        "\n",
        "\n",
        "###### Shape\n",
        "<li> Input: $(N, *)$where $*$ means, any number of additional dimensions\n",
        "\n",
        "<li> Output: $(N, *)$ same shape as the input.\n",
        "\n",
        "\n",
        "### BATCHNORM1D\n",
        "\n",
        "Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D inputs with optional additional channel dimension). Read about it more [here](https://arxiv.org/abs/1502.03167)\n",
        "\n",
        "$y= \\frac{x - E[x]}{\\sqrt(Var[x] + \\epsilon)} * \\gamma + \\beta$\n",
        "\n",
        "\n",
        "The mean and standard-deviation are calculated per-dimension over the mini-batches and $\\gamma$ and $\\beta$ are learnable parameter vectors of size C (where C is the input size). By default, the elements of $\\gamma$ are set to 1 and the elements of $\\beta$ are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).\n",
        "\n",
        "Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default momentum of 0.1.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w49czZrV6sBI"
      },
      "source": [
        "# SIMPLE MODEL DEFINITION\n",
        "class Simple_MLP(nn.Module):\n",
        "    def __init__(self, size_list):\n",
        "        super(Simple_MLP, self).__init__()\n",
        "        layers = []\n",
        "        self.size_list = size_list\n",
        "        for i in range(len(size_list) - 2):\n",
        "            layers.append(nn.Linear(size_list[i],size_list[i+1]))  # the linear function\n",
        "            #layers.append(nn.ReLU())\n",
        "            layers.append(nn.GELU())\n",
        "            layers.append(nn.BatchNorm1d(size_list[i+1]))\n",
        "            #layers.append(nn.Dropout(0.04*i,True))\n",
        "        layers.append(nn.Linear(size_list[-2], size_list[-1]))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB78lXbOBT9z"
      },
      "source": [
        "#### Cross Entropy Loss\n",
        "This criterion combines LogSoftmax and NLLLoss in one single class.It is useful when training a classification problem with C classes. If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes.This is particularly useful when you have an unbalanced training set.\n",
        "\n",
        "\n",
        "#### Adam Optimizer\n",
        "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models.\n",
        "                                             Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n",
        "                                            Adam is relatively easy to configure where the default configuration parameters do well on most problems.\n",
        "\n",
        "\n",
        "\n",
        "#### StepLR \n",
        "Decays the learning rate of each parameter group by gamma every\n",
        "step_size epochs. Notice that such decay can happen simultaneously with\n",
        "other changes to the learning rate from outside this scheduler. When\n",
        "last_epoch=-1, sets initial lr as lr."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI1jGNRM6uuG"
      },
      "source": [
        "model = Simple_MLP([1080, 2048, 2048,  1024, 1024, 1024, 512, 512, 256, 138])\n",
        "model.apply(init_xavier)                     # applying the Xavier weight initialization\n",
        "criterion = nn.CrossEntropyLoss()            # This criterion combines LogSoftmax and NLLLoss in one single class.\n",
        "                                             # It is useful when training a classification problem with C classes. \n",
        "                                             # If provided, the optional argument weight should be a 1D Tensor assigning weight to each of the classes.\n",
        "                                             # This is particularly useful when you have an unbalanced training set.\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())  # Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models.\n",
        "                                            # Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n",
        "                                            # Adam is relatively easy to configure where the default configuration parameters do well on most problems.\n",
        "\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "#scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=2, verbose=True)\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\") # Running the model on GPU is available\n",
        "model.to(device)                            # Saving the model\n",
        "print(model)                                # Printing the Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALK_ZjyG6xtw"
      },
      "source": [
        "# making the epoch function, that tells how many iteration our model runs\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total_predictions = 0.0\n",
        "    correct_predictions = 0.0\n",
        "    model.to(device)\n",
        "    \n",
        "    start_time = time.time()   # saving the time when the model start running\n",
        "    \n",
        "    # Print Learning Rate\n",
        "    \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
        "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
        "        data = data.to(device)\n",
        "        target = target.to(device) # all data & model on same device\n",
        "\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        total_predictions += target.size(0)\n",
        "        correct_predictions += (predicted == target).sum().item()\n",
        "        \n",
        "        loss = criterion(outputs, target)                                        # the loss function\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    scheduler.step()\n",
        "    end_time = time.time()\n",
        "    \n",
        "    running_loss /= len(train_loader)\n",
        "    acc = (correct_predictions/total_predictions)*100.0\n",
        "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')  # printing the Training Loss and the Time required to run\n",
        "    print('Training Accuracy: ', acc, '%')                                       # printing the Training Accuracy\n",
        "    return running_loss,acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQKCyRhm60Tv"
      },
      "source": [
        "def val_model(model, val_loader, criterion):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        model.to(device)\n",
        "\n",
        "        running_loss = 0.0\n",
        "        total_predictions = 0.0\n",
        "        correct_predictions = 0.0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(val_loader):   \n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            outputs = model(data)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_predictions += target.size(0)\n",
        "            correct_predictions += (predicted == target).sum().item()\n",
        "\n",
        "            loss = criterion(outputs, target).detach()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "        running_loss /= len(val_loader)\n",
        "        acc = (correct_predictions/total_predictions)*100.0\n",
        "        print('Testing Loss: ', running_loss)\n",
        "        print('Testing Accuracy: ', acc, '%')\n",
        "        return running_loss, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xr5_z0ij62QJ"
      },
      "source": [
        "def test_model(model, test_loader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        pred = []\n",
        "\n",
        "        for batch_idx, (data) in enumerate(test_loader):   \n",
        "            data = data.to(device)\n",
        "            outputs = model(data)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            pred.append(predicted.cpu().numpy()[0])\n",
        "\n",
        "        return np.array(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0t7uYVL634f"
      },
      "source": [
        "# Using the above mentioned functions here to actually train the model\n",
        "\n",
        "n_epochs = 30     # number of epochs this model will run\n",
        "Train_acc = []    # declared empty arrays in the beginning \n",
        "Train_loss = []   # they will be filled when the code runs\n",
        "Val_loss = []\n",
        "Val_acc = []\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    print('Epoch: ',i+1)\n",
        "    print('LR: ', scheduler.get_lr())\n",
        "    train_loss,acc = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    test_loss, test_acc = val_model(model, val_loader, criterion)\n",
        "    Train_loss.append(train_loss)\n",
        "    Train_acc.append(acc)\n",
        "    Val_loss.append(test_loss)\n",
        "    Val_acc.append(test_acc)\n",
        "    print('='*20)\n",
        "    #scheduler.step(test_acc)\n",
        "    torch.save(model.state_dict(), '/content/drive/My Drive/model1.pt')  # saving the model that we made on the google drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eZ4hgpG6_tf"
      },
      "source": [
        "# finding the prediction of the model\n",
        "\n",
        "pred= test_model(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAUrGuTvGRbi"
      },
      "source": [
        "### Generating the Output csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b90LIKu7CAH"
      },
      "source": [
        "# this code will make a new csv file in the synced Google drive\n",
        "\n",
        "\n",
        "with open('/content/drive/My Drive/monitsharma.csv', 'w') as w: # open a csv file by the name monitsharma in drive and have the permission of w\n",
        "    w.write('id,label\\n')                                       # write the columns id and label\n",
        "    for i in range(len(pred)):                                  # for the range of length of the prediction module\n",
        "            w.write(str(i)+','+str(pred[i])+'\\n')               # append the value in the csv file created by the name monitsharma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QqrZNToGyUN"
      },
      "source": [
        "### Plotting various functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqumPeCJ7Fx_"
      },
      "source": [
        "# ploting the graph between the epoch number and the varying Loss with it \n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(Train_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_9sfQBg7Iin"
      },
      "source": [
        "# Plotting the Value Loss graph\n",
        "plt.title('Val Loss')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.plot(Val_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UIsAjIL7J3P"
      },
      "source": [
        "# Plotting the graph for Epoch and Accuray of our model name it Val Accuracy\n",
        "\n",
        "plt.title('Val Accuracy')\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.plot(Val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6K1T3DQ7Ljm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}